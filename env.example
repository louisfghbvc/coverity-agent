# Coverity Agent Environment Configuration
# Copy this file to .env and fill in your actual values

# ====== NVIDIA NIM Configuration ======
# Primary LLM provider using NVIDIA Inference Microservices
NVIDIA_NIM_API_KEY=your_nim_api_token_here
NVIDIA_NIM_BASE_URL=https://integrate.api.nvidia.com/v1
NVIDIA_NIM_MODEL=nvidia/llama-3.3-nemotron-super-49b-v1
NVIDIA_NIM_MAX_TOKENS=4096
NVIDIA_NIM_TEMPERATURE=0.6
NVIDIA_NIM_TOP_P=0.95
NVIDIA_NIM_FREQUENCY_PENALTY=0.0
NVIDIA_NIM_PRESENCE_PENALTY=0.0
NVIDIA_NIM_STREAMING=true
NVIDIA_NIM_TIMEOUT=30

# ====== Fallback Provider Configuration (Optional) ======
# OpenAI fallback provider
OPENAI_API_KEY=your_openai_key_here

# Anthropic fallback provider  
ANTHROPIC_API_KEY=your_anthropic_key_here

# ====== Pipeline Configuration ======
# Defect analysis settings
DEFECT_ANALYSIS_CACHE_DURATION=24h
ENABLE_MULTIPLE_CANDIDATES=true
NUM_FIX_CANDIDATES=3
CONFIDENCE_THRESHOLD=0.7

# ====== Performance and Cost Settings ======
# Performance optimization
MAX_CONTEXT_LINES=50
TOKEN_LIMIT_PER_DEFECT=2000
ENABLE_PROMPT_COMPRESSION=true

# Cost control
MAX_COST_PER_DEFECT=0.50
DAILY_COST_LIMIT=100.00

# ====== Quality Control ======
# Style and validation settings
ENFORCE_STYLE_CONSISTENCY=true
VALIDATE_SYNTAX=true
SAFETY_CHECKS=true
MAX_FILES_PER_FIX=3

# Confidence thresholds
MIN_CONFIDENCE_FOR_AUTO_APPLY=0.8
MIN_STYLE_SCORE_FOR_AUTO_APPLY=0.7

# ====== Logging and Debugging ======
LOG_LEVEL=INFO
DEBUG_MODE=false
SAVE_RAW_RESPONSES=false

# ====== NIM Rate Limiting ======
NIM_MAX_REQUESTS_PER_MINUTE=60
NIM_RETRY_ATTEMPTS=3
NIM_RETRY_DELAY=1.0

# ====== Cache Configuration ======
CACHE_SIMILAR_DEFECTS=true
CACHE_MAX_SIZE=1000
ENABLE_PERFORMANCE_TRACKING=true 