# LLM Fix Generator Configuration with NVIDIA NIM Integration
# This configuration demonstrates dotenv-based environment variable management

llm_fix_generator:
  # Environment-driven configuration
  load_from_env: true
  env_file_path: ".env"
  
  # Provider configuration with fallback strategy
  providers:
    primary: "nvidia_nim"
    fallback: ["openai", "anthropic"]
    
  # Provider-specific configurations (loaded from environment variables)
  providers_config:
    nvidia_nim:
      # All values loaded from .env file
      api_key: "${NVIDIA_NIM_API_KEY}"
      base_url: "${NVIDIA_NIM_BASE_URL}"
      model: "${NVIDIA_NIM_MODEL}"
      max_tokens: "${NVIDIA_NIM_MAX_TOKENS}"
      temperature: "${NVIDIA_NIM_TEMPERATURE}"
      top_p: "${NVIDIA_NIM_TOP_P}"
      frequency_penalty: "${NVIDIA_NIM_FREQUENCY_PENALTY}"
      presence_penalty: "${NVIDIA_NIM_PRESENCE_PENALTY}"
      timeout: "${NVIDIA_NIM_TIMEOUT}"
      use_streaming: "${NVIDIA_NIM_STREAMING}"
      retry_attempts: "${NIM_RETRY_ATTEMPTS}"
      retry_delay: "${NIM_RETRY_DELAY}"
      max_requests_per_minute: "${NIM_MAX_REQUESTS_PER_MINUTE}"
      estimated_cost_per_1k_tokens: 0.01
      
    openai:
      api_key: "${OPENAI_API_KEY}"
      base_url: "https://api.openai.com/v1"
      model: "gpt-4"
      max_tokens: 2000
      temperature: 0.1
      timeout: 30
      use_streaming: false
      retry_attempts: 3
      retry_delay: 1.0
      max_requests_per_minute: 60
      
    anthropic:
      api_key: "${ANTHROPIC_API_KEY}"
      base_url: "https://api.anthropic.com/v1"
      model: "claude-3-sonnet-20240229"
      max_tokens: 2000
      temperature: 0.1
      timeout: 30
      use_streaming: false
      retry_attempts: 3
      retry_delay: 1.0
      max_requests_per_minute: 60
      
  # Analysis configuration (loaded from environment)
  analysis:
    generate_multiple_candidates: "${ENABLE_MULTIPLE_CANDIDATES}"
    num_candidates: "${NUM_FIX_CANDIDATES}"
    confidence_threshold: "${CONFIDENCE_THRESHOLD}"
    include_reasoning_trace: true
    enable_defect_categorization: true
    include_severity_assessment: true
    max_context_lines: "${MAX_CONTEXT_LINES}"
    include_function_signature: true
    include_surrounding_code: true
    
  # Quality control configuration
  quality:
    enforce_style_consistency: "${ENFORCE_STYLE_CONSISTENCY}"
    style_consistency_threshold: 0.6
    validate_syntax: "${VALIDATE_SYNTAX}"
    safety_checks: "${SAFETY_CHECKS}"
    require_explanation: true
    max_files_per_fix: "${MAX_FILES_PER_FIX}"
    max_lines_per_fix: 100
    min_confidence_for_auto_apply: "${MIN_CONFIDENCE_FOR_AUTO_APPLY}"
    min_style_score_for_auto_apply: "${MIN_STYLE_SCORE_FOR_AUTO_APPLY}"
    
  # Performance optimization
  optimization:
    cache_similar_defects: "${CACHE_SIMILAR_DEFECTS}"
    cache_duration_hours: 24
    cache_max_size: "${CACHE_MAX_SIZE}"
    token_limit_per_defect: "${TOKEN_LIMIT_PER_DEFECT}"
    enable_prompt_compression: "${ENABLE_PROMPT_COMPRESSION}"
    context_window_optimization: true
    enable_performance_tracking: "${ENABLE_PERFORMANCE_TRACKING}"
    log_token_usage: true
    track_generation_time: true
    max_cost_per_defect: "${MAX_COST_PER_DEFECT}"
    daily_cost_limit: "${DAILY_COST_LIMIT}"
    
  # Logging and debugging
  log_level: "${LOG_LEVEL}"
  debug_mode: "${DEBUG_MODE}"
  save_raw_responses: "${SAVE_RAW_RESPONSES}"

# Environment Variables to Set:
# 
# Required for NVIDIA NIM:
#   export NIM_API_ENDPOINT="https://your-nim-endpoint.com/v1/chat/completions"
#   export NIM_API_KEY="your-nvidia-nim-api-key"
#
# Optional for local NIM fallback:
#   export LOCAL_NIM_ENDPOINT="http://localhost:8000/v1/chat/completions"
#   export LOCAL_NIM_API_KEY="local-api-key-if-needed"
#
# Model selection (optional, defaults shown above):
#   export NIM_MODEL="codellama-13b-instruct"
#   export LOCAL_NIM_MODEL="codellama-7b-instruct"

# Usage Example:
# from src.fix_generator import LLMFixGenerator
# 
# # Load from this config file
# generator = LLMFixGenerator.create_with_config_file("config/llm_fix_generator_config.yaml")
# 
# # Or use default configuration
# generator = LLMFixGenerator.create_default()
# 
# # Analyze a defect
# result = generator.analyze_and_fix(parsed_defect, code_context) 